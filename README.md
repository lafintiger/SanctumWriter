# SanctumWriter âœï¸

> **Your private sanctuary for writing with AI â€” 100% Local, 100% Yours**

A local-first markdown editor that uses your own LLMs (Ollama/LM Studio) as a collaborative writing companion. Like Cursor for code, but for prose.

![SanctumWriter](https://img.shields.io/badge/Status-Beta-blue) ![License](https://img.shields.io/badge/License-MIT-green) ![Local](https://img.shields.io/badge/100%25-Local-purple) ![Privacy](https://img.shields.io/badge/Privacy-First-orange)

---

## ğŸ” Why SanctumWriter?

| Feature | SanctumWriter | Other AI Writers |
|---------|---------------|------------------|
| **Privacy** | âœ… 100% local - nothing leaves your machine | âŒ Data sent to cloud servers |
| **Cost** | âœ… Free forever (uses your local LLMs) | âŒ Monthly subscriptions |
| **Internet** | âœ… Works offline | âŒ Requires connection |
| **Your Data** | âœ… Stored locally, you control it | âŒ Stored on company servers |
| **Open Source** | âœ… MIT License | âŒ Usually closed source |

---

## ğŸ†š SanctumWriter vs SanctumWriter Pro

| Feature | SanctumWriter | [SanctumWriter Pro](https://github.com/lafintiger/SanctumWriterPro) |
|---------|---------------|---------------------|
| **LLM Providers** | Ollama, LM Studio (local only) | Local + OpenRouter, OpenAI, Anthropic, Google, xAI |
| **Privacy** | 100% Local | Choice of local or cloud |
| **Cost** | Free | Free + API costs |
| **Best For** | Privacy-focused writers | Writers wanting access to frontier models |
| **Port** | `localhost:3125` | `localhost:3130` |

> ğŸ’¡ **Choose SanctumWriter** if privacy is paramount and you're happy with local models.  
> ğŸ’¡ **Choose [Pro](https://github.com/lafintiger/SanctumWriterPro)** if you need GPT-4, Claude, or other cloud models.

---

## âœ¨ Features

### Core Writing
- ğŸ“ **Rich Markdown Editor** â€” Full-featured editor with syntax highlighting (CodeMirror 6)
- ğŸ‘ï¸ **Live Preview** â€” See rendered markdown as you type
- ğŸ“ **Workspace Browser** â€” Navigate and manage your documents (Obsidian-compatible!)
- ğŸ’¾ **Auto-Save** â€” Never lose your work

### AI Writing Companion
- ğŸ¤– **Agentic Editing** â€” AI directly modifies your document (no copy/paste!)
- ğŸ¯ **Selection-Aware** â€” Highlight text and ask the AI to rewrite just that section
- ğŸ’¬ **Contextual Chat** â€” AI sees your full document and selection
- ğŸ”§ **Hardware Optimization** â€” Auto-detects your GPU and optimizes settings

### Advanced Features
- ğŸ‘¥ **Council of Writers** â€” Multiple AI reviewers analyze your work
- ğŸ” **Research Integration** â€” Search with SearXNG for fact-checking
- ğŸ“Š **Quality Assurance** â€” Hallucination detection, fact verification, AI artifact removal
- ğŸ“‹ **Writing Workflow** â€” Guided checklist from outline to polish
- ğŸ“ˆ **Readability Metrics** â€” Flesch-Kincaid and other scores
- ğŸ¯ **Focus Mode** â€” Distraction-free writing
- ğŸ“š **Citations & Bibliography** â€” Key-based citation management
- ğŸ“¤ **Export** â€” PDF, DOCX, HTML, TXT formats
- ğŸ§  **RAG Knowledge Base** â€” Use your documents as AI context
- ğŸ’­ **Session Memory** â€” AI remembers your writing preferences

---

## ğŸš€ Quick Start

### Prerequisites & Dependencies

SanctumWriter integrates with several local services. Here's what you need:

#### System Requirements

| Requirement | Version | Install Link |
|-------------|---------|--------------|
| **[Node.js](https://nodejs.org)** | 18+ | [nodejs.org](https://nodejs.org) |
| **npm** | Included with Node.js | â€” |

#### LLM Backend (Pick One)

| # | Service | Purpose | Install Link |
|---|---------|---------|--------------|
| 1 | **[Ollama](https://ollama.ai)** â­ | Local LLM inference (recommended) | [ollama.ai](https://ollama.ai) |
| â€” | *OR* **[LM Studio](https://lmstudio.ai)** | Alternative local LLM with GUI | [lmstudio.ai](https://lmstudio.ai) |

#### Optional (For Advanced Features)

| # | Service | Purpose | Install Link | Requires |
|---|---------|---------|--------------|----------|
| 2 | **[Docker Desktop](https://www.docker.com/products/docker-desktop/)** | Container runtime for search services | [docker.com](https://www.docker.com/products/docker-desktop/) | â€” |
| 3 | **[SearXNG](https://github.com/searxng/searxng)** | Privacy-focused web search | [GitHub](https://github.com/searxng/searxng) | Docker |
| 4 | **[Perplexica](https://github.com/ItzCrazyKns/Perplexica)** | AI-powered search with summaries | [GitHub](https://github.com/ItzCrazyKns/Perplexica) | Docker + Ollama |

#### Installation Order

```
REQUIRED (Native Install):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Node.js (for SanctumWriter)
2. Ollama OR LM Studio (for AI) â† Install directly, NOT in Docker

OPTIONAL (Docker-based):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
3. Docker Desktop (only if using search features below)
4. SearXNG (privacy search - runs in Docker)
5. Perplexica (AI search - runs in Docker)

FINALLY:
â”€â”€â”€â”€â”€â”€â”€â”€
6. SanctumWriter
```

> âš ï¸ **Note**: Install Ollama/LM Studio natively on your machine (not in Docker). While Docker versions exist, native installation gives better performance and GPU access.

---

### Setting Up Ollama (Recommended)

```bash
# 1. Install Ollama from https://ollama.ai

# 2. Pull a writing-focused model
ollama pull qwen3:latest

# 3. Start the server (runs on port 11434)
ollama serve
```

### Setting Up LM Studio (Alternative)

1. Download from [lmstudio.ai](https://lmstudio.ai)
2. Load a model (e.g., Llama 3, Mistral, Qwen)
3. Go to **Local Server** tab â†’ **Start Server** (runs on port 1234)

### Setting Up Search Services (Optional)

**SearXNG** (Privacy-focused search):
```bash
# Using Docker
docker run -d --name searxng -p 4000:8080 searxng/searxng
```

**Perplexica** (AI-powered search):
```bash
# Clone and follow setup instructions
git clone https://github.com/ItzCrazyKns/Perplexica.git
cd Perplexica
# See their README for Docker setup (runs on port 3000)
```

---

### Installing SanctumWriter

#### Option A: Standard Installation (npm)

```bash
# Clone the repo
git clone https://github.com/lafintiger/SanctumWriter.git
cd SanctumWriter

# Install dependencies
npm install

# Start the app
npm run dev
```

Open **http://localhost:3125** in your browser.

#### Option B: Docker Installation ğŸ³

**Quick Start (uses Ollama on your host machine):**
```bash
# Clone the repo
git clone https://github.com/lafintiger/SanctumWriter.git
cd SanctumWriter

# Build and run
docker-compose up -d

# View logs
docker-compose logs -f
```

**With Ollama in Docker (no local Ollama needed):**
```bash
# Start app + Ollama container
docker-compose --profile ollama up -d

# Pull a model into the container
docker exec sanctum-ollama ollama pull qwen3:latest
```

**Development with hot-reloading:**
```bash
docker-compose -f docker-compose.dev.yml up
```

| Docker Command | Description |
|----------------|-------------|
| `docker-compose up -d` | Start app (connects to host Ollama) |
| `docker-compose --profile ollama up -d` | Start app + Ollama container |
| `docker-compose down` | Stop all services |
| `docker-compose logs -f` | View live logs |
| `docker-compose build --no-cache` | Rebuild after code changes |

**Environment Variables:**
```bash
# Create .env file for custom settings
OLLAMA_URL=http://host.docker.internal:11434
LMSTUDIO_URL=http://host.docker.internal:1234
DEFAULT_PROVIDER=ollama
DEFAULT_MODEL=llama3
```

> ğŸ’¡ **Tip**: Your documents are persisted in `./documents` folder and LanceDB data in a Docker volume.

---

## ğŸ“– Usage

### Basic Editing
1. Click **+** to create a new document
2. Write markdown in the editor
3. Documents auto-save as you type

### AI Assistance
1. Type a message in the chat panel
2. The AI sees your document and any selected text
3. Ask for help: *"Make this more engaging"* or *"Expand this section"*

### Selection-Based Editing
1. **Highlight text** in the editor
2. Chat shows "Selection active"
3. Ask: *"Rewrite this"* or *"Make it more concise"*
4. AI directly modifies just the selected text

### Council of Writers (Multi-Model Review)
1. Open Settings â†’ Council Configuration
2. Enable reviewers (Style, Clarity, Fact-checker, etc.)
3. Click **Start Council Review**
4. Review suggestions in the Review Document

---

## âš™ï¸ Configuration

### Service URLs (Settings Panel)

Configure your local services in the Settings modal. All URLs are customizable if you use non-default ports:

| Service | Default URL | Purpose | Project |
|---------|-------------|---------|---------|
| Ollama | `http://localhost:11434` | Local LLM inference | [ollama.ai](https://ollama.ai) |
| LM Studio | `http://localhost:1234` | Alternative local LLM | [lmstudio.ai](https://lmstudio.ai) |
| SearXNG | `http://localhost:4000` | Privacy-focused search | [GitHub](https://github.com/searxng/searxng) |
| Perplexica | `http://localhost:3000` | AI-powered search | [GitHub](https://github.com/ItzCrazyKns/Perplexica) |

> ğŸ’¡ **Tip**: If running services in Docker, use `localhost:PORT` â€” SanctumWriter runs on your host machine and can reach Docker containers via localhost.

### Workspace Folder

Set your working directory in Settings â†’ Workspace. Works great with **Obsidian vaults**!

---

## âŒ¨ï¸ Keyboard Shortcuts

| Shortcut | Action |
|----------|--------|
| `Ctrl/Cmd + S` | Save document |
| `Ctrl/Cmd + Z` | Undo |
| `Ctrl/Cmd + Shift + Z` | Redo |
| `Ctrl/Cmd + F` | Find in document |
| `Escape` | Toggle Focus Mode |

---

## ğŸ› ï¸ Tech Stack

- **Framework**: Next.js 14 (App Router)
- **Editor**: CodeMirror 6
- **Styling**: Tailwind CSS
- **State**: Zustand
- **Vector DB**: LanceDB (for RAG)
- **LLM**: Ollama / LM Studio

---

## ğŸ› Troubleshooting

### "Ollama not available"
```bash
# Make sure Ollama is running
ollama serve

# Check it's accessible
curl http://localhost:11434/api/tags
```

### Models not showing
```bash
# Pull a model first
ollama pull qwen3:latest

# Or for a smaller model
ollama pull gemma3:4b
```

### Port conflict
The app runs on port **3125** by default. If you need a different port, modify `package.json`.

---

## ğŸ“„ License

MIT - See [LICENSE](LICENSE) for details.

---

## ğŸ™ Acknowledgments

Built with â¤ï¸ for writers who value their **privacy**.

---

<div align="center">

**SanctumWriter** â€” *Your words. Your sanctuary. Your privacy.*

[Report Bug](https://github.com/lafintiger/SanctumWriter/issues) Â· [Request Feature](https://github.com/lafintiger/SanctumWriter/issues)

</div>
