# SanctumWriter Docker Compose
# Usage:
#   docker-compose up -d              # Start app (connects to host Ollama)
#   docker-compose --profile ollama up -d  # Start app with Ollama container
#
# Access the app at: http://localhost:3125

services:
  # ============================================
  # SanctumWriter Application
  # ============================================
  sanctum-writer:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: sanctum-writer
    ports:
      - "3125:3125"
    volumes:
      # Mount your documents folder for persistence
      - ./documents:/app/documents
      # Optional: mount LanceDB data for RAG persistence
      - sanctum-lancedb:/app/.lancedb
    environment:
      - NODE_ENV=production
      - WORKSPACE_PATH=/app/documents
      # Use host.docker.internal to connect to Ollama running on host
      # Or use 'ollama' service name if running with --profile ollama
      - OLLAMA_URL=${OLLAMA_URL:-http://host.docker.internal:11434}
      - LMSTUDIO_URL=${LMSTUDIO_URL:-http://host.docker.internal:1234}
      - DEFAULT_PROVIDER=${DEFAULT_PROVIDER:-ollama}
      - DEFAULT_MODEL=${DEFAULT_MODEL:-llama3}
    restart: unless-stopped
    # For connecting to host services (Ollama on host)
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3125/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # ============================================
  # Ollama Service (Optional)
  # Start with: docker-compose --profile ollama up -d
  # ============================================
  ollama:
    image: ollama/ollama:latest
    container_name: sanctum-ollama
    profiles:
      - ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    restart: unless-stopped
    # GPU support (uncomment if you have NVIDIA GPU)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

volumes:
  sanctum-lancedb:
    name: sanctum-lancedb
  ollama-data:
    name: sanctum-ollama-data



